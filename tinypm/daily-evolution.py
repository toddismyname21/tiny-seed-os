#!/usr/bin/env python3
"""
TinyPM Daily Evolution Engine
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Runs daily to keep Tiny Seed Farm OS at state-of-the-art.

THE MANTRA:
    NO SHORTCUTS. BEST POSSIBLE. PRODUCTION-READY. ALWAYS IMPROVING.
    We research until we're right. We build to production.
    The system learns from reality, updates from the frontier, and makes
    the next best move for Tiny Seed Farmâ€”before we ask.
    It's not clever; it's dependable.

PROACTIVE DECISION SYSTEM ARCHITECTURE:
    1. Always-current knowledge (auto-ingest changelogs, docs, SOPs, data)
    2. Strong reasoning + tools (plan, call tools, fetch truth, act)
    3. Guardrails + approvals (suggest everything, risky actions need OK)
    4. Evals + monitoring (improve only when metrics say improved)

MODEL ROUTING STRATEGY (Jan 2026):
    - Claude Opus 4.5: Deep writing, planning, analysis, grant narratives
    - GPT-5.2: Tool calling, structured workflows, ops automation
    - Gemini 3 Pro: Vision/multimodal (photos, labels, field issues)

This script:
1. Auto-ingests official docs/changelogs for current knowledge
2. Performs research on latest AI/farm/business techniques
3. Runs evals on real Tiny Seed tasks
4. Promotes improvements only if metrics improve (with rollback)
5. Creates evolution tasks from research findings

Usage:
    python3 daily-evolution.py              # Full cycle
    python3 daily-evolution.py --research   # Research only
    python3 daily-evolution.py --evolve     # Evolution only
    python3 daily-evolution.py --ingest     # Auto-ingest docs
    python3 daily-evolution.py --eval       # Run evaluations
    python3 daily-evolution.py --report     # Show stats

Automation:
    Add to crontab for daily runs:
    0 6 * * * cd ~/Documents/TIny_Seed_OS/tinypm && python3 daily-evolution.py
"""

from __future__ import annotations

import json
import os
import subprocess
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PATHS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
APP_DIR = Path(__file__).parent
PROJECT_ROOT = APP_DIR.parent
RESEARCH_DIR = PROJECT_ROOT / "research"
BOARD_FILE = APP_DIR / "board.json"
EVOLUTION_LOG = RESEARCH_DIR / "EVOLUTION_LOG.md"
PERSONAS_DIR = APP_DIR / "personas"

# Existing Tiny Seed OS Learning System API
# The system already has RLHF-inspired learning - we ENHANCE, not rebuild
API_BASE = "https://script.google.com/macros/s/AKfycbyT60fyrNfmZkgK3z1-ojgISeZBAbBr9Zz50UtSjqSysE5JpB_cAIjp2KFucwREG4qm/exec"

# Existing learning endpoints to leverage:
# - getLearningStats() - Get current learning statistics
# - getDTMLearningData() - Get DTM learning data
# - getLearnedDTM() - Get learned DTM predictions
# - recordPriorityFeedback() - RLHF feedback with seasonal awareness
# - recordCorrectionFeedback() - COS learning feedback

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AUTO-INGEST SOURCES (Official docs/changelogs to stay current)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
AUTO_INGEST_SOURCES = [
    {
        "name": "OpenAI Models",
        "url": "https://platform.openai.com/docs/models",
        "type": "models",
        "check_frequency": "daily"
    },
    {
        "name": "Anthropic Release Notes",
        "url": "https://docs.anthropic.com/en/release-notes",
        "type": "releases",
        "check_frequency": "daily"
    },
    {
        "name": "Google AI Models",
        "url": "https://ai.google.dev/gemini-api/docs/models/gemini",
        "type": "models",
        "check_frequency": "daily"
    },
    {
        "name": "LangChain Updates",
        "url": "https://github.com/langchain-ai/langchain/releases",
        "type": "releases",
        "check_frequency": "weekly"
    },
    {
        "name": "LMSYS Chatbot Arena",
        "url": "https://chat.lmsys.org/?leaderboard",
        "type": "benchmarks",
        "check_frequency": "weekly"
    }
]

# Model routing configuration - match task type to best model
MODEL_ROUTING = {
    "writing": "claude-opus-4.5",      # Deep writing, planning, analysis
    "planning": "claude-opus-4.5",     # Strategy, architecture
    "grants": "claude-opus-4.5",       # Grant narratives, proposals
    "negotiation": "claude-opus-4.5",  # Careful communication
    "tools": "gpt-5.2",                # Tool calling, structured output
    "automation": "gpt-5.2",           # Ops automation, workflows
    "coding": "gpt-5.2",               # Code generation
    "vision": "gemini-3-pro",          # Images, photos, labels
    "multimodal": "gemini-3-pro",      # Mixed media
    "field_photos": "gemini-3-pro",    # Field issue inspection
}

# Evaluation tasks - real Tiny Seed work to measure improvement
EVAL_TASKS = [
    {
        "name": "email_drafting",
        "description": "Draft response to CSA member complaint",
        "metrics": ["tone_match", "helpfulness", "accuracy"],
        "baseline_score": 0.0  # Will be set after first run
    },
    {
        "name": "csa_issue_handling",
        "description": "Triage and respond to CSA issue report",
        "metrics": ["response_time", "resolution_quality", "customer_satisfaction"],
        "baseline_score": 0.0
    },
    {
        "name": "irrigation_decision",
        "description": "Recommend irrigation schedule given weather forecast",
        "metrics": ["water_efficiency", "crop_health_prediction", "cost_optimization"],
        "baseline_score": 0.0
    },
    {
        "name": "harvest_prediction",
        "description": "Predict harvest date for given planting",
        "metrics": ["accuracy_days", "confidence_calibration"],
        "baseline_score": 0.0
    },
    {
        "name": "labor_scheduling",
        "description": "Create optimal crew schedule for the week",
        "metrics": ["coverage", "fairness", "cost"],
        "baseline_score": 0.0
    }
]

# Research topics to cycle through
RESEARCH_TOPICS = [
    {
        "topic": "Agentic AI 2026",
        "queries": [
            "agentic AI orchestration 2026",
            "multi-agent systems best practices",
            "AI agent memory systems",
            "trust calibration AI agents"
        ],
        "category": "AI"
    },
    {
        "topic": "Farm Intelligence",
        "queries": [
            "AI crop planning algorithms 2026",
            "predictive yield forecasting",
            "farm labor optimization AI",
            "weather integration agriculture"
        ],
        "category": "FARM"
    },
    {
        "topic": "Business AI",
        "queries": [
            "customer retention AI small business",
            "predictive analytics SMB",
            "AI marketing automation 2026",
            "inventory optimization machine learning"
        ],
        "category": "BUSINESS"
    },
    {
        "topic": "Technical Stack",
        "queries": [
            "Google Apps Script performance 2026",
            "Progressive Web Apps best practices",
            "GitHub Pages optimization",
            "API caching strategies"
        ],
        "category": "TECH"
    },
    {
        "topic": "UX Innovation",
        "queries": [
            "dashboard design patterns 2026",
            "mobile first farm software",
            "AI powered UX",
            "voice interface business apps"
        ],
        "category": "UX"
    },
    {
        "topic": "Food Safety Tech",
        "queries": [
            "food traceability software 2026",
            "FSMA compliance automation",
            "PHI tracking technology",
            "farm food safety AI"
        ],
        "category": "COMPLIANCE"
    },
    {
        "topic": "Self-Improving Systems",
        "queries": [
            "self-improving AI systems",
            "continuous learning production systems",
            "MLOps small business",
            "automated system optimization"
        ],
        "category": "META"
    }
]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# UTILITIES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def ensure_dirs():
    """Create necessary directories."""
    RESEARCH_DIR.mkdir(exist_ok=True)

def load_board() -> dict:
    """Load the task board."""
    if BOARD_FILE.exists():
        with open(BOARD_FILE, "r") as f:
            return json.load(f)
    return {"version": "1.0", "project": "Tiny Seed Farm OS", "tasks": [], "next_id": 1}

def save_board(board: dict):
    """Save the task board."""
    with open(BOARD_FILE, "w") as f:
        json.dump(board, f, indent=2)

def get_day_of_week() -> int:
    """Get current day of week (0=Monday)."""
    return datetime.now().weekday()

def get_todays_topic() -> dict:
    """Get the research topic for today (cycles through topics)."""
    day = get_day_of_week()
    return RESEARCH_TOPICS[day % len(RESEARCH_TOPICS)]

def log_evolution(message: str):
    """Append to evolution log."""
    ensure_dirs()
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")

    if not EVOLUTION_LOG.exists():
        EVOLUTION_LOG.write_text("# Tiny Seed Farm OS - Evolution Log\n\nTracking daily improvements to keep the system state-of-the-art.\n\n---\n\n")

    with open(EVOLUTION_LOG, "a") as f:
        f.write(f"\n## {timestamp}\n\n{message}\n\n---\n")

def create_evolution_task(title: str, description: str, role: str = "builder", priority: str = "high") -> dict:
    """Create a new evolution task."""
    board = load_board()
    task_id = board.get("next_id", 1)
    now = datetime.now().strftime("%Y-%m-%d")

    task = {
        "id": f"EVO-{task_id:03d}",
        "title": f"[EVOLUTION] {title}",
        "description": description,
        "role": role,
        "priority": priority,
        "status": "pending",
        "context": [],
        "created": now,
        "updated": now,
        "source": "daily-evolution"
    }

    board["tasks"].append(task)
    board["next_id"] = task_id + 1
    save_board(board)

    return task

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RESEARCH ENGINE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def perform_research():
    """
    Perform today's research.

    This launches a Claude agent with the researcher persona
    to search the web and compile findings.
    """
    topic = get_todays_topic()
    today = datetime.now().strftime("%Y-%m-%d")

    print(f"\n{'â•' * 60}")
    print(f"  DAILY RESEARCH: {topic['topic']}")
    print(f"  Category: {topic['category']}")
    print(f"  Date: {today}")
    print(f"{'â•' * 60}\n")

    # Load researcher persona
    researcher_prompt = (PERSONAS_DIR / "researcher.md").read_text()

    # Build research request
    queries = "\n".join(f"- {q}" for q in topic['queries'])

    research_prompt = f"""
{researcher_prompt}

## TODAY'S RESEARCH ASSIGNMENT

**Topic:** {topic['topic']}
**Category:** {topic['category']}
**Date:** {today}

**Search Queries to Use:**
{queries}

**Your Task:**
1. Search the web for each query
2. Find the most recent (2025-2026) and authoritative sources
3. Identify techniques that could improve Tiny Seed Farm OS
4. Write a research report following the format in your persona
5. Save the report to: /Users/samanthapollack/Documents/TIny_Seed_OS/research/{today}_{topic['category'].lower()}_research.md

Focus on ACTIONABLE improvements, not theoretical concepts.
What can we actually implement to make the system smarter?
"""

    # Save prompt for manual use if Claude CLI isn't available
    prompt_file = APP_DIR / ".research-prompt.md"
    prompt_file.write_text(research_prompt)

    print(f"Research prompt saved to: {prompt_file}")
    print(f"Topic: {topic['topic']}")
    print(f"Queries: {len(topic['queries'])}")
    print()

    # Try to launch Claude
    claude_cmd = find_claude()
    if claude_cmd:
        print(f"Launching Claude researcher...")
        try:
            subprocess.run([
                claude_cmd,
                "--system-prompt", research_prompt,
                "--cwd", str(PROJECT_ROOT),
            ])
        except Exception as e:
            print(f"Error launching Claude: {e}")
            print(f"Run manually: claude --system-prompt \"$(cat {prompt_file})\"")
    else:
        print("Claude CLI not found.")
        print(f"Run research manually with: claude --system-prompt \"$(cat {prompt_file})\"")

    log_evolution(f"### Research Performed\n- Topic: {topic['topic']}\n- Category: {topic['category']}\n- Queries: {len(topic['queries'])}")

def find_claude() -> Optional[str]:
    """Find Claude CLI."""
    candidates = [
        "claude",
        os.path.expanduser("~/.claude/local/claude"),
        "/usr/local/bin/claude",
        "/opt/homebrew/bin/claude",
    ]
    for c in candidates:
        try:
            result = subprocess.run(["which", c] if "/" not in c else ["test", "-x", c],
                                  capture_output=True, timeout=5)
            if result.returncode == 0:
                return c
        except:
            pass
    return None

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EVOLUTION ENGINE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def check_recent_research() -> List[Path]:
    """Find research reports from the last 7 days."""
    cutoff = datetime.now() - timedelta(days=7)
    recent = []

    if RESEARCH_DIR.exists():
        for f in RESEARCH_DIR.glob("*.md"):
            if f.name.startswith("20"):  # Date-prefixed files
                try:
                    date_str = f.name[:10]
                    file_date = datetime.strptime(date_str, "%Y-%m-%d")
                    if file_date >= cutoff:
                        recent.append(f)
                except:
                    pass

    return sorted(recent, reverse=True)

def extract_recommendations(report_path: Path) -> List[dict]:
    """Extract actionable recommendations from a research report."""
    content = report_path.read_text()
    recommendations = []

    # Simple parsing - look for table rows or bullet points under "Recommendations"
    in_recommendations = False
    for line in content.split("\n"):
        if "recommendation" in line.lower() and "#" in line:
            in_recommendations = True
            continue
        if in_recommendations:
            if line.startswith("#"):
                break
            if "|" in line and "HIGH" in line.upper():
                parts = [p.strip() for p in line.split("|")]
                if len(parts) >= 3:
                    recommendations.append({
                        "priority": "high",
                        "title": parts[1] if len(parts) > 1 else "Unknown",
                        "source": report_path.name
                    })
            elif line.strip().startswith("- ") and ("implement" in line.lower() or "add" in line.lower() or "create" in line.lower()):
                recommendations.append({
                    "priority": "medium",
                    "title": line.strip("- ").strip(),
                    "source": report_path.name
                })

    return recommendations

def run_evolution():
    """
    Run the evolution cycle:
    1. Check recent research
    2. Extract recommendations
    3. Create evolution tasks
    4. Log progress
    """
    print(f"\n{'â•' * 60}")
    print(f"  EVOLUTION CYCLE")
    print(f"  Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    print(f"{'â•' * 60}\n")

    # Find recent research
    reports = check_recent_research()
    print(f"Recent research reports: {len(reports)}")

    if not reports:
        print("No recent research found. Run --research first.")
        return

    # Extract recommendations
    all_recommendations = []
    for report in reports:
        recs = extract_recommendations(report)
        all_recommendations.extend(recs)
        print(f"  {report.name}: {len(recs)} recommendations")

    print(f"\nTotal recommendations: {len(all_recommendations)}")

    # Check what's already in the board
    board = load_board()
    existing_titles = {t.get("title", "").lower() for t in board.get("tasks", [])}

    # Create tasks for new recommendations
    created = 0
    for rec in all_recommendations:
        title = rec["title"][:60]
        if f"[evolution] {title}".lower() not in existing_titles:
            task = create_evolution_task(
                title=title,
                description=f"From research: {rec['source']}\n\nImplement this improvement to keep Tiny Seed Farm OS state-of-the-art.",
                priority=rec["priority"]
            )
            print(f"  Created: {task['id']} - {title}")
            created += 1

    print(f"\nTasks created: {created}")

    # Log
    log_evolution(f"### Evolution Cycle\n- Reports analyzed: {len(reports)}\n- Recommendations found: {len(all_recommendations)}\n- Tasks created: {created}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EXISTING LEARNING SYSTEM INTEGRATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def fetch_existing_learning_stats() -> Optional[dict]:
    """
    Fetch learning statistics from the EXISTING Tiny Seed OS learning system.
    This leverages the already-built RLHF-inspired learning that's been working.
    """
    import urllib.request
    import urllib.error

    try:
        url = f"{API_BASE}?action=getLearningStats"
        req = urllib.request.Request(url, headers={'User-Agent': 'TinyPM-Evolution'})
        with urllib.request.urlopen(req, timeout=30) as response:
            data = json.loads(response.read().decode())
            return data if data.get('success') else None
    except Exception as e:
        print(f"  (Could not fetch learning stats: {e})")
        return None

def fetch_dtm_learning_data() -> Optional[dict]:
    """
    Fetch DTM (Days to Maturity) learning data from existing system.
    The system already learns from actual harvest dates vs predictions.
    """
    import urllib.request

    try:
        url = f"{API_BASE}?action=getDTMLearningData"
        req = urllib.request.Request(url, headers={'User-Agent': 'TinyPM-Evolution'})
        with urllib.request.urlopen(req, timeout=30) as response:
            data = json.loads(response.read().decode())
            return data if data.get('success') else None
    except Exception as e:
        print(f"  (Could not fetch DTM data: {e})")
        return None

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# METRICS & REPORTING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def show_evolution_stats():
    """Show evolution statistics including existing learning system."""
    print(f"\n{'â•' * 60}")
    print(f"  EVOLUTION & LEARNING STATISTICS")
    print(f"{'â•' * 60}\n")

    # --- EXISTING LEARNING SYSTEM STATS ---
    print("EXISTING LEARNING SYSTEM (Already Built):")
    print("â”€" * 40)

    learning_stats = fetch_existing_learning_stats()
    if learning_stats:
        print(f"  Total Feedback Recorded: {learning_stats.get('totalFeedback', 0)}")
        print(f"  Applied Learnings: {learning_stats.get('appliedLearnings', 0)}")
        patterns = learning_stats.get('patterns', [])
        if patterns:
            print(f"  Patterns Learned: {len(patterns)}")
            for p in patterns[:3]:  # Show top 3
                print(f"    - {p}")
    else:
        print("  (API not reachable - stats unavailable)")

    dtm_data = fetch_dtm_learning_data()
    if dtm_data:
        print(f"\n  DTM Learning:")
        print(f"    Crops Tracked: {dtm_data.get('cropsTracked', 0)}")
        print(f"    Prediction Accuracy: {dtm_data.get('accuracy', 'N/A')}")
    print()

    # --- EVOLUTION ENGINE STATS ---
    print("EVOLUTION ENGINE (Enhancement Layer):")
    print("â”€" * 40)

    # Research stats
    if RESEARCH_DIR.exists():
        reports = list(RESEARCH_DIR.glob("*.md"))
        print(f"  Research Reports: {len(reports)}")
        recent = check_recent_research()
        print(f"  Last 7 days: {len(recent)}")
    else:
        print("  Research Reports: 0")

    # Task stats
    board = load_board()
    tasks = board.get("tasks", [])
    evo_tasks = [t for t in tasks if t.get("source") == "daily-evolution" or "[EVOLUTION]" in t.get("title", "")]

    print(f"\n  Evolution Tasks: {len(evo_tasks)}")
    pending = sum(1 for t in evo_tasks if t.get("status") == "pending")
    progress = sum(1 for t in evo_tasks if t.get("status") == "in_progress")
    done = sum(1 for t in evo_tasks if t.get("status") == "done")
    print(f"    Pending: {pending}")
    print(f"    In Progress: {progress}")
    print(f"    Done: {done}")

    if evo_tasks and done > 0:
        print(f"\n  Completion Rate: {done}/{len(evo_tasks)} ({100*done//len(evo_tasks)}%)")

    # Topic coverage
    print("\n  Research Topic Schedule (Daily Rotation):")
    for i, topic in enumerate(RESEARCH_TOPICS):
        day_name = ["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"][i % 7]
        marker = "â†’" if i == get_day_of_week() else " "
        print(f"    {marker} {day_name}: {topic['topic']} ({topic['category']})")

    print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AUTO-INGEST ENGINE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def run_auto_ingest():
    """
    Auto-ingest official docs/changelogs to maintain current knowledge.

    This keeps the system aware of:
    - New model releases (GPT-5.2, Claude 4.5, Gemini 3)
    - API changes and deprecations
    - Best practice updates
    - Benchmark results
    """
    import urllib.request

    print(f"\n{'â•' * 60}")
    print(f"  AUTO-INGEST: Updating Knowledge Base")
    print(f"  Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    print(f"{'â•' * 60}\n")

    ingested = []
    today = datetime.now().strftime("%Y-%m-%d")
    day_of_week = get_day_of_week()

    for source in AUTO_INGEST_SOURCES:
        # Check frequency
        if source["check_frequency"] == "weekly" and day_of_week != 0:
            print(f"  â­ {source['name']} (weekly, skipping)")
            continue

        print(f"  ğŸ“¥ {source['name']}...")

        try:
            req = urllib.request.Request(
                source["url"],
                headers={'User-Agent': 'TinyPM-Evolution/1.0'}
            )
            with urllib.request.urlopen(req, timeout=30) as response:
                content = response.read().decode('utf-8', errors='ignore')[:10000]  # First 10KB

            # Save to research folder
            safe_name = source["name"].lower().replace(" ", "_")
            ingest_file = RESEARCH_DIR / f"ingest_{safe_name}_{today}.md"

            with open(ingest_file, "w") as f:
                f.write(f"# Auto-Ingest: {source['name']}\n")
                f.write(f"**Date:** {today}\n")
                f.write(f"**Source:** {source['url']}\n")
                f.write(f"**Type:** {source['type']}\n\n")
                f.write("## Raw Content (First 10KB)\n\n")
                f.write(content[:5000])  # Truncate for storage
                f.write("\n\n---\n*Auto-ingested by TinyPM Evolution Engine*\n")

            ingested.append(source["name"])
            print(f"    âœ“ Saved to {ingest_file.name}")

        except Exception as e:
            print(f"    âœ— Failed: {e}")

    print(f"\n  Ingested: {len(ingested)} sources")
    log_evolution(f"### Auto-Ingest\n- Sources checked: {len(AUTO_INGEST_SOURCES)}\n- Successfully ingested: {len(ingested)}\n- Sources: {', '.join(ingested)}")

    return ingested

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EVALUATION ENGINE (Improve only when metrics improve)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
EVAL_RESULTS_FILE = RESEARCH_DIR / "eval_results.json"

def load_eval_results() -> dict:
    """Load previous evaluation results."""
    if EVAL_RESULTS_FILE.exists():
        with open(EVAL_RESULTS_FILE, "r") as f:
            return json.load(f)
    return {"runs": [], "baselines": {}}

def save_eval_results(results: dict):
    """Save evaluation results."""
    ensure_dirs()
    with open(EVAL_RESULTS_FILE, "w") as f:
        json.dump(results, f, indent=2)

def run_evaluations():
    """
    Run evaluations on real Tiny Seed tasks.

    This measures if system changes actually improve performance.
    Promotion to production only happens if scores improve.
    """
    print(f"\n{'â•' * 60}")
    print(f"  EVALUATION ENGINE")
    print(f"  Running evals on real Tiny Seed tasks")
    print(f"{'â•' * 60}\n")

    results = load_eval_results()
    today = datetime.now().strftime("%Y-%m-%d")
    run_results = {"date": today, "tasks": []}

    for task in EVAL_TASKS:
        print(f"  ğŸ“Š {task['name']}: {task['description']}")

        # For now, we'll use the existing learning system to measure
        # In production, this would call actual evaluation endpoints
        task_result = {
            "name": task["name"],
            "metrics": {},
            "overall_score": 0.0,
            "status": "pending_implementation"
        }

        # Try to get real metrics from the API
        try:
            import urllib.request
            # Check if this task has metrics in the learning system
            url = f"{API_BASE}?action=getTaskMetrics&task={task['name']}"
            req = urllib.request.Request(url, headers={'User-Agent': 'TinyPM-Eval'})
            with urllib.request.urlopen(req, timeout=10) as response:
                data = json.loads(response.read().decode())
                if data.get("success") and data.get("metrics"):
                    task_result["metrics"] = data["metrics"]
                    task_result["overall_score"] = data.get("score", 0.0)
                    task_result["status"] = "measured"
        except:
            pass  # API endpoint may not exist yet

        # Compare to baseline
        baseline_key = task["name"]
        baseline = results.get("baselines", {}).get(baseline_key, 0.0)

        if task_result["overall_score"] > 0:
            if baseline == 0:
                # First run - set baseline
                results.setdefault("baselines", {})[baseline_key] = task_result["overall_score"]
                task_result["comparison"] = "baseline_set"
                print(f"    â†’ Baseline set: {task_result['overall_score']:.2f}")
            elif task_result["overall_score"] > baseline:
                improvement = ((task_result["overall_score"] - baseline) / baseline) * 100
                task_result["comparison"] = f"improved_{improvement:.1f}%"
                print(f"    â†’ IMPROVED: {baseline:.2f} â†’ {task_result['overall_score']:.2f} (+{improvement:.1f}%)")
            elif task_result["overall_score"] < baseline:
                regression = ((baseline - task_result["overall_score"]) / baseline) * 100
                task_result["comparison"] = f"regressed_{regression:.1f}%"
                print(f"    â†’ REGRESSED: {baseline:.2f} â†’ {task_result['overall_score']:.2f} (-{regression:.1f}%)")
            else:
                task_result["comparison"] = "unchanged"
                print(f"    â†’ Unchanged: {task_result['overall_score']:.2f}")
        else:
            print(f"    â†’ Pending implementation (no metrics available)")

        run_results["tasks"].append(task_result)

    results["runs"].append(run_results)

    # Keep only last 30 runs
    if len(results["runs"]) > 30:
        results["runs"] = results["runs"][-30:]

    save_eval_results(results)

    # Summary
    measured = sum(1 for t in run_results["tasks"] if t["status"] == "measured")
    improved = sum(1 for t in run_results["tasks"] if "improved" in t.get("comparison", ""))
    regressed = sum(1 for t in run_results["tasks"] if "regressed" in t.get("comparison", ""))

    print(f"\n  Summary:")
    print(f"    Tasks evaluated: {len(run_results['tasks'])}")
    print(f"    Measured: {measured}")
    print(f"    Improved: {improved}")
    print(f"    Regressed: {regressed}")

    if regressed > 0:
        print(f"\n  âš ï¸  REGRESSION DETECTED - Review changes before promoting")
    elif improved > 0:
        print(f"\n  âœ… IMPROVEMENTS DETECTED - Safe to promote")

    log_evolution(f"### Evaluation Run\n- Tasks: {len(EVAL_TASKS)}\n- Measured: {measured}\n- Improved: {improved}\n- Regressed: {regressed}")

    return run_results

def get_model_recommendation(task_type: str) -> str:
    """
    Get the recommended model for a task type.
    Uses the MODEL_ROUTING configuration.
    """
    return MODEL_ROUTING.get(task_type, "claude-opus-4.5")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def main():
    ensure_dirs()

    if len(sys.argv) > 1:
        arg = sys.argv[1]
        if arg == "--research":
            perform_research()
        elif arg == "--evolve":
            run_evolution()
        elif arg == "--ingest":
            run_auto_ingest()
        elif arg == "--eval":
            run_evaluations()
        elif arg == "--report":
            show_evolution_stats()
        elif arg == "--help":
            print(__doc__)
        else:
            print(f"Unknown argument: {arg}")
            print("Use --help for usage info")
    else:
        # Full cycle: ingest + research + eval + evolution
        print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  TINY SEED FARM OS - DAILY EVOLUTION ENGINE                               â•‘
â•‘                                                                           â•‘
â•‘  NO SHORTCUTS. BEST POSSIBLE. PRODUCTION-READY. ALWAYS IMPROVING.         â•‘
â•‘                                                                           â•‘
â•‘  We research until we're right. We build to production.                   â•‘
â•‘  The system learns from reality, updates from the frontier,               â•‘
â•‘  and makes the next best moveâ€”before you ask.                             â•‘
â•‘  It's not clever; it's dependable.                                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
        # Step 1: Auto-ingest latest docs/changelogs
        print("\n[STEP 1/4] AUTO-INGEST: Updating knowledge base...")
        run_auto_ingest()

        print("\n" + "â”€" * 60 + "\n")

        # Step 2: Research today's topic
        print("[STEP 2/4] RESEARCH: Finding frontier improvements...")
        perform_research()

        print("\n" + "â”€" * 60 + "\n")

        # Step 3: Run evaluations
        print("[STEP 3/4] EVALUATE: Measuring current performance...")
        run_evaluations()

        print("\n" + "â”€" * 60 + "\n")

        # Step 4: Create evolution tasks
        print("[STEP 4/4] EVOLVE: Creating improvement tasks...")
        run_evolution()

        print("\n" + "â”€" * 60 + "\n")

        show_evolution_stats()

if __name__ == "__main__":
    main()
